# ğŸŒ¾ Pipeline de Indicadores EconÃ´micos Brasileiros

Pipeline de dados para extraÃ§Ã£o, armazenamento e anÃ¡lise de indicadores econÃ´micos e agrÃ­colas brasileiros, utilizando arquitetura Lakehouse com Apache Iceberg.
<img width="2816" height="1536" alt="resumo-readmeimagem" src="https://github.com/user-attachments/assets/14305023-e28b-4130-96c7-074e0f1e2117" />

## ğŸ“‹ Ãndice

- [VisÃ£o Geral](#visÃ£o-geral)
- [Arquitetura](#arquitetura)
- [1. ExtraÃ§Ã£o de Dados - APIs Python](#1-extraÃ§Ã£o-de-dados---apis-python)
  - [1.1 CEPEA - Indicadores AgrÃ­colas](#11-cepea---indicadores-agrÃ­colas)
  - [1.2 BCB - Banco Central do Brasil](#12-bcb---banco-central-do-brasil)
  - [1.3 IPEA - Instituto de Pesquisa EconÃ´mica](#13-ipea---instituto-de-pesquisa-econÃ´mica)
  - [1.4 CONAB - Hortifruti (Prohort)](#14-conab---hortifruti-prohort)
- [2. Lakehouse com Apache Iceberg](#2-lakehouse-com-apache-iceberg)
- [3. OrquestraÃ§Ã£o com Airflow](#3-orquestraÃ§Ã£o-com-airflow)
- [4. VisualizaÃ§Ã£o com DBeaver](#4-visualizaÃ§Ã£o-com-dbeaver)
- [Aprendizados e LiÃ§Ãµes](#aprendizados-e-liÃ§Ãµes)
- [Como Executar](#como-executar)

---

## VisÃ£o Geral

Este projeto implementa um pipeline completo de dados para coletar indicadores econÃ´micos de diversas fontes brasileiras e armazenÃ¡-los em um Lakehouse moderno.

**Fontes de Dados:**
| Fonte | Tipo | Dados |
|-------|------|-------|
| CEPEA/ESALQ | Web Scraping | PreÃ§os agrÃ­colas (boi, soja, milho, cafÃ©, etc.) |
| BCB | API REST | Indicadores econÃ´micos (dÃ³lar, SELIC, IPCA) |
| IPEA | API REST | SÃ©ries histÃ³ricas econÃ´micas |
| CONAB | Download Direto | PreÃ§os de hortifruti das CEASAs |

**Stack TecnolÃ³gico:**
- **ExtraÃ§Ã£o:** Python (requests, pandas)
- **Armazenamento:** Apache Iceberg + MinIO (S3)
- **Processamento:** Apache Spark
- **OrquestraÃ§Ã£o:** Apache Airflow
- **VisualizaÃ§Ã£o:** DBeaver

---

## Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           FONTES DE DADOS                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   CEPEA     â”‚    BCB      â”‚    IPEA     â”‚           CONAB                â”‚
â”‚ (scraping)  â”‚  (API)      â”‚   (API)     â”‚      (download TXT)            â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚             â”‚                   â”‚
       â–¼             â–¼             â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        EXTRACTORS (Python)                              â”‚
â”‚  cepea_scraper.py â”‚ bcb_client.py â”‚ ipea_client.py â”‚ conab_client.py   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚             â”‚             â”‚                   â”‚
       â–¼             â–¼             â–¼                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         PARQUET FILES                                    â”‚
â”‚                        /opt/spark/data/                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SPARK + ICEBERG (Bronze Layer)                        â”‚
â”‚                     load_bronze_iceberg.py                               â”‚
â”‚                        (MERGE/Upsert)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           MinIO (S3)                                     â”‚
â”‚                    s3://warehouse/bronze/                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚   cepea_    â”‚    bcb_     â”‚   ipea_     â”‚   conab_    â”‚              â”‚
â”‚  â”‚ indicadores â”‚ indicadores â”‚ indicadores â”‚ indicadores â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                                   â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CONSUMO / VISUALIZAÃ‡ÃƒO                               â”‚
â”‚              DBeaver (SQL) â”‚ Power BI â”‚ Jupyter                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. ExtraÃ§Ã£o de Dados - APIs Python

### 1.1 CEPEA - Indicadores AgrÃ­colas

**Fonte:** Centro de Estudos AvanÃ§ados em Economia Aplicada (ESALQ/USP)

**MÃ©todo:** Web Scraping

**URL Base:** `https://cepea.esalq.usp.br/br/indicador/`

**Indicadores disponÃ­veis:**
- Boi Gordo, Bezerro
- Soja, Milho, Trigo
- CafÃ© ArÃ¡bica, CafÃ© Robusta
- AÃ§Ãºcar, Etanol
- AlgodÃ£o, Arroz, FeijÃ£o
- Frango, SuÃ­no
- Leite, Ovos
- Mandioca

**Estrutura do cÃ³digo:**
```python
class CepeaScraper:
    def __init__(self):
        self.base_url = "https://cepea.esalq.usp.br/br/indicador/"
        self.indicators = {...}  # Mapeamento de indicadores
    
    def extract_indicator(self, indicator: str) -> pd.DataFrame:
        # Faz request HTTP e parseia HTML
        # Retorna DataFrame com: date, value_brl, value_usd, var_day, var_month
```

**Aprendizados CEPEA:**
- âš ï¸ Site nÃ£o tem API oficial - depende de scraping
- âš ï¸ Estrutura HTML pode mudar sem aviso
- âš ï¸ MÃºltiplas tabelas por indicador (Tabela 1, 2, 3) - importante incluir `indicator_name` na chave do MERGE
- âœ… Dados diÃ¡rios atualizados
- âœ… PreÃ§os em BRL e USD

---

### 1.2 BCB - Banco Central do Brasil

**Fonte:** Sistema Gerenciador de SÃ©ries Temporais (SGS)

**MÃ©todo:** API REST oficial

**URL Base:** `https://api.bcb.gov.br/dados/serie/bcdata.sgs.{codigo}/dados`

**SÃ©ries utilizadas:**
| CÃ³digo | DescriÃ§Ã£o |
|--------|-----------|
| 1 | DÃ³lar comercial (venda) |
| 433 | IPCA - VariaÃ§Ã£o mensal |
| 4390 | Taxa SELIC |
| 7326 | PIB mensal |
| 24363 | DÃ­vida lÃ­quida do setor pÃºblico |

**Estrutura do cÃ³digo:**
```python
class BCBClient:
    BASE_URL = "https://api.bcb.gov.br/dados/serie/bcdata.sgs.{}/dados"
    
    def get_series(self, series_code: int, start_date: str = None) -> pd.DataFrame:
        url = self.BASE_URL.format(series_code)
        params = {"formato": "json"}
        if start_date:
            params["dataInicial"] = start_date
        
        response = requests.get(url, params=params)
        return pd.DataFrame(response.json())
```

**Aprendizados BCB:**
- âœ… API oficial, estÃ¡vel e bem documentada
- âœ… Formato JSON limpo
- âœ… NÃ£o requer autenticaÃ§Ã£o
- âœ… Permite filtro por data inicial/final
- âš ï¸ Rate limiting em requisiÃ§Ãµes muito frequentes
- âš ï¸ Algumas sÃ©ries tÃªm frequÃªncia diferente (diÃ¡ria, mensal, anual)

**DocumentaÃ§Ã£o:** https://dadosabertos.bcb.gov.br/

---

### 1.3 IPEA - Instituto de Pesquisa EconÃ´mica

**Fonte:** IPEA Data

**MÃ©todo:** API REST

**URL Base:** `http://www.ipeadata.gov.br/api/odata4/`

**SÃ©ries utilizadas:**
| CÃ³digo | DescriÃ§Ã£o |
|--------|-----------|
| GM366_ERC366 | Taxa de cÃ¢mbio comercial |
| PRECOS366_IGPDI366 | IGP-DI |
| BM366_PIB366 | PIB nominal |

**Estrutura do cÃ³digo:**
```python
class IPEAClient:
    BASE_URL = "http://www.ipeadata.gov.br/api/odata4/"
    
    def get_series(self, series_code: str) -> pd.DataFrame:
        # Primeiro busca metadados da sÃ©rie
        metadata_url = f"{self.BASE_URL}Metadados('{series_code}')"
        
        # Depois busca os valores
        values_url = f"{self.BASE_URL}Metadados('{series_code}')/Valores"
        
        response = requests.get(values_url)
        data = response.json()
        return pd.DataFrame(data['value'])
```

**Aprendizados IPEA:**
- âœ… API OData4 bem estruturada
- âœ… Metadados ricos (descriÃ§Ã£o, fonte, unidade)
- âš ï¸ CÃ³digos de sÃ©ries nÃ£o sÃ£o intuitivos
- âš ï¸ Algumas sÃ©ries descontinuadas
- âš ï¸ Performance pode variar

**DocumentaÃ§Ã£o:** http://www.ipeadata.gov.br/api/

---

### 1.4 CONAB - Hortifruti (Prohort)

**Fonte:** Companhia Nacional de Abastecimento - Programa Prohort

**MÃ©todo:** Download direto de arquivo TXT

**URL:** `https://portaldeinformacoes.conab.gov.br/downloads/arquivos/ProhortDiario.txt`

**Dados disponÃ­veis:**
- PreÃ§os diÃ¡rios de hortifruti
- 48 produtos (frutas e hortaliÃ§as)
- 43 CEASAs de todo Brasil
- HistÃ³rico desde 2022

**Estrutura do arquivo:**
```
municipio_ceasa;cod_ibge_municipio;uf_ceasa;dsc_ceasa;dsc_produto;sig_unidade_medida;data_preco;preco_diario
ARAÃ‡ATUBA-SP;3502804;SP;CEAGESP - ARACATUBA;ABACATE;KG;2022/07/08 00:00:00.000;5.8
```

**Estrutura do cÃ³digo:**
```python
class ConabProhortClient:
    ENDPOINTS = {
        "diario": "https://portaldeinformacoes.conab.gov.br/downloads/arquivos/ProhortDiario.txt",
        "mensal": "https://portaldeinformacoes.conab.gov.br/downloads/arquivos/ProhortMensal.txt"
    }
    
    def extract_diario(self, produtos=None, estados=None) -> pd.DataFrame:
        # Download do arquivo completo
        # Leitura com encoding latin-1
        # Limpeza e padronizaÃ§Ã£o
        # Filtros opcionais
```

**Aprendizados CONAB:**
- âœ… Dados oficiais do governo
- âœ… Download simples (HTTP GET)
- âœ… ~900.000 registros disponÃ­veis
- âœ… Funciona em qualquer ambiente (nÃ£o precisa Selenium!)
- âš ï¸ Arquivo grande (~150MB) - demora para baixar
- âš ï¸ Encoding `latin-1` (nÃ£o UTF-8)
- âš ï¸ Dados podem ter duplicatas - necessÃ¡rio deduplicar
- âš ï¸ Timestamps em formato `NANOS` causam erro no Spark - converter para `microseconds`

**Importante - Por que nÃ£o usamos Selenium:**

Inicialmente consideramos usar Selenium para extrair dados do site HF Brasil (CEPEA Hortifruti), mas identificamos problemas:

| Aspecto | Selenium | CONAB (Download) |
|---------|----------|------------------|
| Complexidade | Alta (Chrome, WebDriver) | Baixa (HTTP GET) |
| ManutenÃ§Ã£o | Alta (site pode mudar) | Baixa |
| Performance | Lenta | RÃ¡pida |
| EMR/ProduÃ§Ã£o | DifÃ­cil | FÃ¡cil |
| Confiabilidade | Baixa | Alta |

A CONAB oferece os **mesmos dados** em formato muito mais acessÃ­vel.

---

## 2. Lakehouse com Apache Iceberg

### Por que migrar para Iceberg?

| Problema com Parquet puro | SoluÃ§Ã£o com Iceberg |
|---------------------------|---------------------|
| Sem controle de transaÃ§Ãµes | ACID transactions |
| DifÃ­cil fazer UPDATE/DELETE | MERGE (upsert) nativo |
| Sem versionamento | Time travel |
| Schema rÃ­gido | Schema evolution |
| DifÃ­cil particionar | Particionamento transparente |

### Arquitetura Docker

```yaml
services:
  minio:        # Storage S3-compatible
  iceberg-rest: # CatÃ¡logo Iceberg
  spark-master: # Processamento
  spark-worker: # Workers
  spark-thrift: # SQL interface (JDBC)
```

### Estrutura das Tabelas Bronze

**iceberg.bronze.cepea_indicadores:**
```sql
CREATE TABLE iceberg.bronze.cepea_indicadores (
    date DATE,
    value_brl DOUBLE,
    var_day_pct DOUBLE,
    var_month_pct DOUBLE,
    value_usd DOUBLE,
    region STRING,
    indicator STRING,
    indicator_name STRING,  -- Importante para diferenciar tabelas!
    unit STRING,
    source STRING,
    extracted_at TIMESTAMP,
    _loaded_at TIMESTAMP
)
```

**iceberg.bronze.conab_indicadores:**
```sql
CREATE TABLE iceberg.bronze.conab_indicadores (
    date DATE,
    product STRING,
    price DOUBLE,
    unit STRING,
    ceasa_name STRING,
    municipality STRING,
    state STRING,
    ibge_code STRING,
    source STRING,
    data_type STRING,
    extracted_at TIMESTAMP,
    _loaded_at TIMESTAMP
)
```

### Processo de Carga (MERGE)

O script `load_bronze_iceberg.py` implementa:

1. **ValidaÃ§Ã£o de Schema:** Verifica se o Parquet Ã© compatÃ­vel com Spark
2. **DetecÃ§Ã£o de Duplicatas:** Identifica chaves duplicadas antes do MERGE
3. **DeduplicaÃ§Ã£o AutomÃ¡tica:** Remove duplicatas se encontradas
4. **MERGE (Upsert):** Atualiza existentes, insere novos

```python
# ConfiguraÃ§Ã£o das chaves de MERGE
tables = [
    {
        "file": "cepea_indicadores.parquet",
        "table": "cepea_indicadores",
        "merge_keys": ["date", "indicator", "indicator_name"]  # Chave composta!
    },
    {
        "file": "conab_indicadores.parquet",
        "table": "conab_indicadores",
        "merge_keys": ["date", "product", "ceasa_name"]
    },
    # ...
]
```

### Erros Comuns e SoluÃ§Ãµes

| Erro | Causa | SoluÃ§Ã£o |
|------|-------|---------|
| `MERGE_CARDINALITY_VIOLATION` | Duplicatas na chave | Adicionar mais colunas Ã  chave ou deduplicar |
| `Illegal Parquet type: TIMESTAMP(NANOS)` | Pandas salva timestamp em nanos | Usar `coerce_timestamps='us'` no to_parquet() |
| `SCHEMA_NOT_FOUND: default` | Thrift nÃ£o encontra namespace | Criar `iceberg.default` ou conectar sem database |

---

## 3. OrquestraÃ§Ã£o com Airflow

### Estrutura da DAG

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚check_containers â”‚ --> â”‚   extractions   â”‚ --> â”‚    validate     â”‚
â”‚                 â”‚     â”‚ (extract_conab) â”‚     â”‚   extractions   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                        â”‚
                                                        â–¼
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   verify_load   â”‚ <-- â”‚  load_bronze    â”‚
                        â”‚                 â”‚     â”‚    iceberg      â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Tasks

| Task | DescriÃ§Ã£o | Comando |
|------|-----------|---------|
| `check_containers` | Verifica se Docker estÃ¡ rodando | `docker ps \| grep spark-master` |
| `extract_conab` | Extrai dados da CONAB | `docker exec spark-master python conab_prohort_client.py` |
| `validate_extractions` | Valida arquivos gerados | `ls -lh *.parquet` |
| `load_bronze_iceberg` | Carrega no Iceberg com MERGE | `spark-submit load_bronze_iceberg.py` |
| `verify_load` | Conta registros finais | `spark-sql -e "SELECT COUNT(*)..."` |

### ConfiguraÃ§Ã£o

```python
with DAG(
    dag_id='pipeline_indicadores_economicos',
    schedule_interval='0 6 * * *',  # Diariamente Ã s 6h
    start_date=datetime(2025, 1, 1),
    catchup=False,
    tags=['iceberg', 'bronze', 'indicadores'],
) as dag:
```

### Aprendizados Airflow

- âš ï¸ **Jinja Templates:** Evitar `{{ }}` em comandos bash (conflita com Jinja do Airflow)
- âš ï¸ **Docker Socket:** Precisa de permissÃ£o (`chmod 666 /var/run/docker.sock`)
- âš ï¸ **Volumes:** Montar `/var/run/docker.sock` e pastas do projeto no container Airflow
- âœ… **BashOperator:** Simples e eficiente para executar comandos Docker
- âœ… **TaskGroup:** Organiza tasks relacionadas visualmente

---

## 4. VisualizaÃ§Ã£o com DBeaver

### ConexÃ£o

| Campo | Valor |
|-------|-------|
| Tipo | Apache Spark / Apache Hive |
| Host | localhost |
| Port | 10000 |
| Database | *(deixar vazio)* |
| Authentication | No Authentication |

### Queries Ãšteis

```sql
-- Listar tabelas
SHOW TABLES IN iceberg.bronze;

-- Contagem por tabela
SELECT 'conab' as fonte, COUNT(*) as registros FROM iceberg.bronze.conab_indicadores
UNION ALL
SELECT 'bcb', COUNT(*) FROM iceberg.bronze.bcb_indicadores
UNION ALL
SELECT 'ipea', COUNT(*) FROM iceberg.bronze.ipea_indicadores
UNION ALL
SELECT 'cepea', COUNT(*) FROM iceberg.bronze.cepea_indicadores;

-- Top produtos CONAB
SELECT product, COUNT(*) as registros, ROUND(AVG(price), 2) as preco_medio
FROM iceberg.bronze.conab_indicadores
GROUP BY product
ORDER BY registros DESC
LIMIT 20;

-- EvoluÃ§Ã£o do preÃ§o da banana
SELECT date, AVG(price) as preco_medio
FROM iceberg.bronze.conab_indicadores
WHERE product = 'BANANA'
GROUP BY date
ORDER BY date;
```

---

## Aprendizados e LiÃ§Ãµes

### Sobre APIs de Dados Brasileiras

1. **Prefira APIs oficiais** quando disponÃ­veis (BCB, IPEA)
2. **Evite Selenium** - sempre procure alternativas (CONAB vs HF Brasil)
3. **Encoding:** Dados brasileiros frequentemente usam `latin-1`, nÃ£o `UTF-8`
4. **DocumentaÃ§Ã£o:** APIs governamentais nem sempre sÃ£o bem documentadas

### Sobre Apache Iceberg

1. **MERGE keys:** Escolha cuidadosamente - erros de cardinalidade sÃ£o comuns
2. **Timestamps:** Pandas usa nanosegundos, Spark espera microsegundos
3. **Schema evolution:** Iceberg facilita, mas planejar schema inicial bem ajuda

### Sobre Arquitetura

1. **Separar responsabilidades:** Airflow orquestra, Spark processa, Iceberg armazena
2. **Validar antes de carregar:** Evita erros em produÃ§Ã£o
3. **Logs detalhados:** Essenciais para debug

---

## Como Executar

### PrÃ©-requisitos

- Docker e Docker Compose
- WSL2 (Windows) ou Linux
- ~8GB RAM disponÃ­vel

### 1. Subir o Lakehouse

```bash
cd ~/Jornada/iceberg-dbt-project
docker compose up -d
docker ps  # Verificar containers
```

### 2. Executar extraÃ§Ã£o manual

```bash
# CONAB
docker exec spark-master python /opt/spark/scripts/conab_prohort_client.py \
    --output /opt/spark/data/conab_indicadores.parquet

# Carregar no Iceberg
docker exec spark-master spark-submit /opt/spark/scripts/load_bronze_iceberg.py
```

### 3. Subir o Airflow

```bash
cd ~/Jornada/airflow_b
docker compose up -d

# Verificar DAG
docker compose exec airflow-webserver airflow dags list | grep indicadores
```

### 4. Acessar interfaces

| ServiÃ§o | URL | Credenciais |
|---------|-----|-------------|
| Airflow | - | - |
| MinIO | - | - |
| Spark UI | - | - |

### 5. Conectar DBeaver

- Host: `localhost`
- Port: `10000`
- Tipo: Apache Spark

---

## Estrutura de Arquivos

```
~/Jornada/
â”œâ”€â”€ iceberg-dbt-project/
â”‚   â”œâ”€â”€ docker-compose.yml      # Stack Iceberg + Spark + MinIO
â”‚   â”œâ”€â”€ scripts/
â”‚   â”‚   â”œâ”€â”€ conab_prohort_client.py   # Extrator CONAB
â”‚   â”‚   â””â”€â”€ load_bronze_iceberg.py    # Loader com MERGE
â”‚   â”œâ”€â”€ data/                   # Arquivos Parquet
â”‚   â””â”€â”€ spark/
â”‚       â””â”€â”€ Dockerfile          # Imagem Spark customizada
â”‚
â””â”€â”€ airflow_b/
    â”œâ”€â”€ docker-compose.yaml     # Stack Airflow
    â””â”€â”€ dags/
        â””â”€â”€ pipeline_indicadores_dag.py  # DAG de orquestraÃ§Ã£o
```

---

## PrÃ³ximos Passos

- [ ] Adicionar camada Silver (transformaÃ§Ãµes dbt)
- [ ] Adicionar camada Gold (agregaÃ§Ãµes para BI)
- [ ] Conectar Power BI via ODBC
- [ ] Implementar alertas de falha no Airflow
- [ ] Adicionar mais fontes de dados (B3, CVM)

---

## Autor

**Braulio Campos**  
Senior Data Engineer

---

*Ãšltima atualizaÃ§Ã£o: Janeiro/2026*
